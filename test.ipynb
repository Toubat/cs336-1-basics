{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)\n",
    "print(type(utf8_encoded))\n",
    "list(utf8_encoded)\n",
    "\n",
    "print(len(test_string))\n",
    "print(len(utf8_encoded))\n",
    "\n",
    "print(utf8_encoded.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c3cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.escape(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb975e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in re.finditer(PAT, \"some text that i'll pre-tokenize\"):\n",
    "    print(f'\"{match.group()}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.bpe.utils import PAT\n",
    "import regex as re\n",
    "\n",
    "text = \"\"\"\n",
    "```py\n",
    "from cs336_basics.bpe.utils import PAT\n",
    "import regex as re\n",
    "\n",
    "for match in re.finditer(PAT, \"some text that i'll pre-tokenize\"):\n",
    "    print(f'\"{match.group()}\"')\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for match in re.finditer(PAT, text):\n",
    "    print(f'\"{match.group()}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.bpe.train import get_pretoken_counts\n",
    "\n",
    "counts = get_pretoken_counts(\"data/TinyStoriesV2-GPT4-train.txt\", [\"<|endoftext|>\"])\n",
    "len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e417a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "(1, 2, 3, 4): 5,\n",
    "(2, 4, 4, 5): 4,\n",
    "(1, 2, 3, 5): 3,\n",
    "(1, 2, 4, 5): 2,\n",
    "(1, 3, 4, 5): 1,    \n",
    "\n",
    "1 2 -> [ (1, 2, 3, 4), (1, 2, 3, 5), (1, 2, 4, 5) ]\n",
    "\n",
    "---\n",
    "\n",
    "(12, 3, 4): 5,\n",
    "(2, 4, 4, 5): 4,\n",
    "(12, 3, 5): 3,\n",
    "(12, 4, 5): 2,\n",
    "(1, 3, 4, 5): 1,\n",
    "\n",
    "1 2 -> [ (12, 3, 4), (12, 3, 5), (2, 4, 5) ]\n",
    "\n",
    "\n",
    "2 3 -> [ (1, 2, 3, 4), (1, 2, 3, 5) ]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenRef:\n",
    "    tokens: list[bytes]\n",
    "    count: int\n",
    "\n",
    "\n",
    "class BytePair(NamedTuple):\n",
    "    left: bytes\n",
    "    right: bytes\n",
    "\n",
    "\n",
    "# Got from pretokenization\n",
    "list token_refs (list[TokenRef]) token_refs\n",
    "\n",
    "# Compute below 2 data structures from token_refs\n",
    "- dict byte_pair BytePair -> total_count (int) bp_to_counts\n",
    "- dict byte_pair BytePair -> token_ref_ids (set[int]) bp_to_token_ref_ids\n",
    "\n",
    "while less than vocab_size:\n",
    "    bp = BytePair with highest count\n",
    "    add it to the vocabulary\n",
    "\n",
    "    token_refs_with_bp = find list of token refs contain this bp (use bp_to_token_ref_ids and token_refs)\n",
    "\n",
    "    for each token_ref in token_refs_with_bp:\n",
    "        compute curr_bp_to_counts for this token_ref\n",
    "        perform merging on pretokenized text chunk\n",
    "        compute next_bp_to_counts for this token_ref\n",
    "\n",
    "        for each bp in curr_bp_to_counts:\n",
    "            next_count = \n",
    "            if bp not in next_bp_to_counts:\n",
    "                remove token_ref id from bp_to_token_ref_ids[bp]\n",
    "\n",
    "            curr_count = curr_bp_to_counts[bp]\n",
    "            next_count = 0 if bp not in next_bp_to_counts else next_bp_to_counts[bp]\n",
    "            bp_to_counts[bp] += next_count - curr_count\n",
    "            !assert bp_to_counts[bp] never be negative\n",
    "        \n",
    "    bp_count, bp_ref_count = bp_to_counts[bp], len(bp_to_token_ref_ids[bp])\n",
    "    !assert bp_count == bp_ref_count == 0\n",
    "\n",
    "    remove bp from bp_to_counts\n",
    "    remove bp from bp_to_token_ref_ids\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
